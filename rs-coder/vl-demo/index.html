<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Qwen2.5-VL + Piper TTS Demo</title>
    <style>
        * {
            box-sizing: border-box;
            margin: 0;
            padding: 0;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: #1a1a2e;
            color: #eee;
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            max-width: 1400px;
            margin: 0 auto;
        }

        h1 {
            text-align: center;
            margin-bottom: 20px;
            color: #00d4ff;
        }

        .main-layout {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
        }

        .panel {
            background: #16213e;
            border-radius: 12px;
            padding: 20px;
            border: 1px solid #0f3460;
        }

        .panel h2 {
            margin-bottom: 15px;
            color: #00d4ff;
            font-size: 1.2rem;
        }

        .video-container {
            position: relative;
            background: #000;
            border-radius: 8px;
            overflow: hidden;
            aspect-ratio: 4/3;
        }

        #webcam {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }

        #canvas {
            display: none;
        }

        .snapshot-preview {
            margin-top: 10px;
            text-align: center;
        }

        #snapshot {
            max-width: 200px;
            border-radius: 4px;
            border: 2px solid #0f3460;
        }

        .controls {
            margin-top: 15px;
        }

        .control-group {
            margin-bottom: 15px;
        }

        .control-group label {
            display: block;
            margin-bottom: 5px;
            color: #aaa;
            font-size: 0.9rem;
        }

        textarea {
            width: 100%;
            padding: 12px;
            border: 1px solid #0f3460;
            border-radius: 8px;
            background: #1a1a2e;
            color: #eee;
            font-size: 14px;
            resize: vertical;
            min-height: 80px;
        }

        textarea:focus {
            outline: none;
            border-color: #00d4ff;
        }

        input[type="text"], input[type="number"], select {
            width: 100%;
            padding: 10px 12px;
            border: 1px solid #0f3460;
            border-radius: 8px;
            background: #1a1a2e;
            color: #eee;
            font-size: 14px;
        }

        input:focus, select:focus {
            outline: none;
            border-color: #00d4ff;
        }

        .button-row {
            display: flex;
            gap: 10px;
            margin-top: 15px;
        }

        button {
            flex: 1;
            padding: 12px 20px;
            border: none;
            border-radius: 8px;
            font-size: 14px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.2s;
        }

        .btn-primary {
            background: #00d4ff;
            color: #1a1a2e;
        }

        .btn-primary:hover {
            background: #00b8e6;
        }

        .btn-primary:disabled {
            background: #555;
            color: #888;
            cursor: not-allowed;
        }

        .btn-secondary {
            background: #0f3460;
            color: #eee;
        }

        .btn-secondary:hover {
            background: #1a4a7a;
        }

        .btn-danger {
            background: #e94560;
            color: #fff;
        }

        .btn-danger:hover {
            background: #d63050;
        }

        .btn-success {
            background: #00c853;
            color: #fff;
        }

        .btn-success:hover {
            background: #00a844;
        }

        .autosend-controls {
            display: flex;
            align-items: center;
            gap: 10px;
            margin-top: 10px;
        }

        .autosend-controls input[type="number"] {
            width: 100px;
        }

        .autosend-controls span {
            color: #aaa;
            font-size: 0.9rem;
        }

        .response-panel {
            display: flex;
            flex-direction: column;
            height: 100%;
        }

        .response-box {
            flex: 1;
            background: #1a1a2e;
            border: 1px solid #0f3460;
            border-radius: 8px;
            padding: 15px;
            overflow-y: auto;
            min-height: 200px;
            max-height: 350px;
            white-space: pre-wrap;
            font-family: 'Monaco', 'Menlo', monospace;
            font-size: 14px;
            line-height: 1.5;
        }

        .status-bar {
            margin-top: 15px;
            padding: 10px;
            background: #1a1a2e;
            border-radius: 8px;
            font-size: 0.85rem;
            color: #888;
        }

        .status-bar.error {
            color: #e94560;
            background: rgba(233, 69, 96, 0.1);
        }

        .status-bar.success {
            color: #00c853;
            background: rgba(0, 200, 83, 0.1);
        }

        .status-bar.loading {
            color: #00d4ff;
            background: rgba(0, 212, 255, 0.1);
        }

        .config-row {
            display: flex;
            gap: 15px;
            margin-bottom: 15px;
        }

        .config-row .control-group {
            flex: 1;
            margin-bottom: 0;
        }

        .streaming-indicator {
            display: inline-block;
            width: 8px;
            height: 8px;
            background: #00c853;
            border-radius: 50%;
            margin-right: 8px;
            animation: pulse 1s infinite;
        }

        @keyframes pulse {
            0%, 100% { opacity: 1; }
            50% { opacity: 0.5; }
        }

        .hidden {
            display: none !important;
        }

        /* Debug Stats Panel */
        .debug-panel {
            margin-top: 15px;
            background: #0d1b2a;
            border: 1px solid #1b4332;
            border-radius: 8px;
            padding: 15px;
        }

        .debug-panel h3 {
            color: #52b788;
            font-size: 0.95rem;
            margin-bottom: 12px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .debug-panel h3::before {
            content: '';
            display: inline-block;
            width: 8px;
            height: 8px;
            background: #52b788;
            border-radius: 50%;
        }

        .stats-grid {
            display: grid;
            grid-template-columns: repeat(3, 1fr);
            gap: 10px;
        }

        .stat-item {
            background: #1a1a2e;
            padding: 10px;
            border-radius: 6px;
            border: 1px solid #0f3460;
        }

        .stat-label {
            font-size: 0.7rem;
            color: #888;
            text-transform: uppercase;
            letter-spacing: 0.5px;
            margin-bottom: 4px;
        }

        .stat-value {
            font-size: 1rem;
            font-weight: 600;
            color: #00d4ff;
            font-family: 'Monaco', 'Menlo', monospace;
        }

        .stat-value.highlight {
            color: #52b788;
        }

        .stat-value.warning {
            color: #ffc107;
        }

        .stat-value.error {
            color: #e94560;
        }

        /* Avatar Panel */
        .avatar-container {
            display: flex;
            justify-content: center;
            align-items: center;
            background: #1a1a2e;
            border-radius: 16px;
            padding: 15px;
            margin-bottom: 15px;
            min-height: 220px;
        }

        .avatar-svg {
            width: 130px;
            height: 195px;
            border-radius: 12px;
            filter: drop-shadow(0 4px 12px rgba(0,0,0,0.4));
        }

        /* TTS Panel */
        .tts-panel {
            margin-top: 15px;
            background: #1a0d2e;
            border: 1px solid #4a1b6d;
            border-radius: 8px;
            padding: 15px;
        }

        .tts-panel h3 {
            color: #b388ff;
            font-size: 0.95rem;
            margin-bottom: 12px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .tts-panel h3::before {
            content: 'ðŸ”Š';
            font-size: 1rem;
        }

        .tts-controls {
            display: flex;
            gap: 10px;
            align-items: center;
            flex-wrap: wrap;
        }

        .tts-controls label {
            color: #aaa;
            font-size: 0.85rem;
        }

        .tts-controls input[type="checkbox"] {
            width: 18px;
            height: 18px;
            cursor: pointer;
        }

        .tts-controls select {
            width: auto;
            min-width: 120px;
        }

        .tts-status {
            margin-top: 10px;
            font-size: 0.85rem;
            color: #b388ff;
        }

        .tts-status.speaking {
            color: #00c853;
        }

        .tts-status.error {
            color: #e94560;
        }

        .audio-queue {
            margin-top: 10px;
            display: flex;
            gap: 5px;
            flex-wrap: wrap;
        }

        .audio-chunk {
            background: #4a1b6d;
            padding: 4px 8px;
            border-radius: 4px;
            font-size: 0.75rem;
            color: #b388ff;
        }

        .audio-chunk.playing {
            background: #00c853;
            color: #fff;
        }

        .audio-chunk.queued {
            background: #0f3460;
            color: #888;
        }

        .resolution-select {
            display: flex;
            gap: 10px;
            align-items: center;
        }

        .resolution-select select {
            flex: 1;
        }

        .resolution-info {
            font-size: 0.8rem;
            color: #888;
            margin-top: 5px;
        }

        @media (max-width: 900px) {
            .main-layout {
                grid-template-columns: 1fr;
            }
            .stats-grid {
                grid-template-columns: repeat(2, 1fr);
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Qwen2.5-VL + Piper TTS Demo</h1>

        <div class="config-row">
            <div class="control-group">
                <label for="apiUrl">VLM API Endpoint</label>
                <input type="text" id="apiUrl" value="">
            </div>
            <div class="control-group">
                <label for="ttsUrl">TTS API Endpoint</label>
                <input type="text" id="ttsUrl" value="">
            </div>
            <div class="control-group">
                <label for="modelName">Model Name</label>
                <input type="text" id="modelName" value="Qwen/Qwen2.5-VL-7B-Instruct">
            </div>
        </div>

        <div class="main-layout">
            <div class="panel">
                <h2>Camera Input</h2>
                <div class="video-container">
                    <video id="webcam" autoplay playsinline></video>
                </div>
                <canvas id="canvas"></canvas>

                <div class="snapshot-preview">
                    <img id="snapshot" class="hidden" alt="Last captured frame">
                </div>

                <div class="controls">
                    <div class="control-group">
                        <label for="cameraSelect">Camera</label>
                        <select id="cameraSelect">
                            <option value="">Loading cameras...</option>
                        </select>
                    </div>

                    <div class="control-group">
                        <label for="resolution">Image Resolution</label>
                        <div class="resolution-select">
                            <select id="resolution">
                                <option value="320x240">320x240 (QVGA) - Fastest</option>
                                <option value="640x480" selected>640x480 (VGA) - Balanced</option>
                                <option value="800x600">800x600 (SVGA)</option>
                                <option value="1280x720">1280x720 (HD 720p)</option>
                                <option value="1920x1080">1920x1080 (Full HD)</option>
                            </select>
                        </div>
                        <div class="resolution-info" id="resolutionInfo">Current: 640x480 | Est. payload: ~50KB</div>
                    </div>

                    <div class="control-group">
                        <label for="prompt">Prompt</label>
                        <textarea id="prompt" placeholder="Describe what you see in the image...">What do you see in this image? Describe it in one or two sentences.</textarea>
                    </div>

                    <div class="button-row">
                        <button id="sendBtn" class="btn-primary">Send</button>
                        <button id="autosendBtn" class="btn-success">Start Autosend</button>
                    </div>

                    <div class="autosend-controls">
                        <label for="interval">Interval:</label>
                        <input type="number" id="interval" value="5000" min="500" step="100">
                        <span>ms</span>
                    </div>
                </div>
            </div>

            <div class="panel response-panel">
                <h2>LLM Response</h2>

                <!-- Animated Avatar -->
                <div class="avatar-container">
                    <svg class="avatar-svg" viewBox="0 0 120 160" id="avatarSvg">
                        <defs>
                            <!-- Background gradient - soft warm sunset -->
                            <linearGradient id="bgGradient" x1="0%" y1="0%" x2="100%" y2="100%">
                                <stop offset="0%" style="stop-color:#4a3c6e"/>
                                <stop offset="40%" style="stop-color:#6b4a7a"/>
                                <stop offset="70%" style="stop-color:#8a5a6a"/>
                                <stop offset="100%" style="stop-color:#c47a6a"/>
                            </linearGradient>
                            <!-- Soft bokeh/light effect -->
                            <radialGradient id="bokeh1" cx="20%" cy="25%" r="30%">
                                <stop offset="0%" style="stop-color:#ffffff; stop-opacity:0.15"/>
                                <stop offset="100%" style="stop-color:#ffffff; stop-opacity:0"/>
                            </radialGradient>
                            <radialGradient id="bokeh2" cx="80%" cy="60%" r="25%">
                                <stop offset="0%" style="stop-color:#ffd4b8; stop-opacity:0.12"/>
                                <stop offset="100%" style="stop-color:#ffd4b8; stop-opacity:0"/>
                            </radialGradient>
                            <radialGradient id="bokeh3" cx="10%" cy="80%" r="20%">
                                <stop offset="0%" style="stop-color:#b8d4ff; stop-opacity:0.1"/>
                                <stop offset="100%" style="stop-color:#b8d4ff; stop-opacity:0"/>
                            </radialGradient>
                            <!-- Gradient for hair -->
                            <linearGradient id="hairGradient" x1="0%" y1="0%" x2="0%" y2="100%">
                                <stop offset="0%" style="stop-color:#5a3d2b"/>
                                <stop offset="50%" style="stop-color:#3d2817"/>
                                <stop offset="100%" style="stop-color:#2a1a0f"/>
                            </linearGradient>
                            <!-- Hair highlight -->
                            <linearGradient id="hairHighlight" x1="0%" y1="0%" x2="100%" y2="0%">
                                <stop offset="0%" style="stop-color:#6b4c35"/>
                                <stop offset="50%" style="stop-color:#7a5a42"/>
                                <stop offset="100%" style="stop-color:#6b4c35"/>
                            </linearGradient>
                            <!-- Gradient for face -->
                            <radialGradient id="faceGradient" cx="50%" cy="35%" r="65%">
                                <stop offset="0%" style="stop-color:#fce4d4"/>
                                <stop offset="70%" style="stop-color:#f0d0b8"/>
                                <stop offset="100%" style="stop-color:#e8c4a8"/>
                            </radialGradient>
                            <!-- Lip gradient -->
                            <linearGradient id="lipGradient" x1="0%" y1="0%" x2="0%" y2="100%">
                                <stop offset="0%" style="stop-color:#d4847a"/>
                                <stop offset="100%" style="stop-color:#b86b62"/>
                            </linearGradient>
                            <!-- Eye gradient for depth -->
                            <radialGradient id="irisGradient" cx="30%" cy="30%" r="70%">
                                <stop offset="0%" style="stop-color:#7a9e8a"/>
                                <stop offset="100%" style="stop-color:#4a6e5a"/>
                            </radialGradient>
                            <!-- Blouse/top gradient -->
                            <linearGradient id="blouseGradient" x1="0%" y1="0%" x2="0%" y2="100%">
                                <stop offset="0%" style="stop-color:#5a7a9a"/>
                                <stop offset="50%" style="stop-color:#4a6a8a"/>
                                <stop offset="100%" style="stop-color:#3a5a7a"/>
                            </linearGradient>
                            <!-- Blouse highlight -->
                            <linearGradient id="blouseHighlight" x1="0%" y1="0%" x2="100%" y2="0%">
                                <stop offset="0%" style="stop-color:#6a8aaa"/>
                                <stop offset="50%" style="stop-color:#7a9aba"/>
                                <stop offset="100%" style="stop-color:#6a8aaa"/>
                            </linearGradient>
                        </defs>

                        <!-- Background -->
                        <rect x="0" y="0" width="120" height="160" fill="url(#bgGradient)"/>
                        <!-- Soft bokeh lights in background -->
                        <ellipse cx="25" cy="40" rx="30" ry="30" fill="url(#bokeh1)"/>
                        <ellipse cx="95" cy="100" rx="25" ry="25" fill="url(#bokeh2)"/>
                        <ellipse cx="15" cy="140" rx="20" ry="20" fill="url(#bokeh3)"/>
                        <ellipse cx="100" cy="30" rx="15" ry="15" fill="url(#bokeh1)"/>

                        <!-- Shoulders - these stay static -->
                        <g id="shouldersGroup">
                            <!-- Long hair behind shoulders (static - starts at head level ~50, connected) -->
                            <path d="M22 50 C16 75 14 100 18 135 Q26 152 42 160 L42 160 L36 120 C26 100 24 70 22 50 Z" fill="url(#hairGradient)"/>
                            <path d="M98 50 C104 75 106 100 102 135 Q94 152 78 160 L78 160 L84 120 C94 100 96 70 98 50 Z" fill="url(#hairGradient)"/>
                            <!-- Hair strands for realism -->
                            <path d="M24 58 C20 85 22 115 28 145" stroke="#4a3020" stroke-width="0.8" fill="none" opacity="0.3"/>
                            <path d="M28 55 C24 80 25 110 32 140" stroke="#5a4030" stroke-width="0.6" fill="none" opacity="0.25"/>
                            <path d="M96 58 C100 85 98 115 92 145" stroke="#4a3020" stroke-width="0.8" fill="none" opacity="0.3"/>
                            <path d="M92 55 C96 80 95 110 88 140" stroke="#5a4030" stroke-width="0.6" fill="none" opacity="0.25"/>

                            <!-- SKIN: Small area of shoulders/collarbone visible in V-neckline -->
                            <path d="M40 136 Q50 134 60 136 Q70 134 80 136 Q72 142 60 145 Q48 142 40 136 Z" fill="url(#faceGradient)"/>
                            <!-- Collarbone shadows -->
                            <path d="M44 137 Q50 136 56 137" stroke="#dbb8a0" stroke-width="1" fill="none" opacity="0.3"/>
                            <path d="M76 137 Q70 136 64 137" stroke="#dbb8a0" stroke-width="1" fill="none" opacity="0.3"/>

                            <!-- BLOUSE: Covers shoulders completely with V-neckline -->
                            <path d="M-5 160 Q5 142 25 135 Q38 132 45 135 L45 160 Z" fill="url(#blouseGradient)"/>
                            <path d="M125 160 Q115 142 95 135 Q82 132 75 135 L75 160 Z" fill="url(#blouseGradient)"/>
                            <!-- V-neckline blouse center -->
                            <path d="M45 135 Q52 142 60 148 Q68 142 75 135 L75 160 L45 160 Z" fill="url(#blouseGradient)"/>
                            <!-- Blouse neckline edge -->
                            <path d="M45 135 Q52 142 60 148 Q68 142 75 135" stroke="#4a6a8a" stroke-width="0.8" fill="none" opacity="0.6"/>
                            <!-- Shoulder seams -->
                            <path d="M28 137 Q15 145 5 152" stroke="#4a6a8a" stroke-width="0.8" fill="none" opacity="0.5"/>
                            <path d="M92 137 Q105 145 115 152" stroke="#4a6a8a" stroke-width="0.8" fill="none" opacity="0.5"/>
                            <!-- Blouse folds -->
                            <path d="M52 148 Q55 155 54 160" stroke="#3a5a7a" stroke-width="1" fill="none" opacity="0.4"/>
                            <path d="M68 148 Q65 155 66 160" stroke="#3a5a7a" stroke-width="1" fill="none" opacity="0.4"/>
                        </g>

                        <!-- Head group - everything that moves with head -->
                        <g id="headGroup">
                            <!-- Solid hair background - blocks background from showing through head AND neck -->
                            <ellipse cx="60" cy="55" rx="42" ry="40" fill="#2a1a0f"/>
                            <!-- Hair behind neck area - extends down to cover behind neck -->
                            <path d="M25 70 L25 140 Q35 145 60 145 Q85 145 95 140 L95 70 Z" fill="#2a1a0f"/>
                            <!-- Hair back/top of head - connects to side hair at ~50 -->
                            <path d="M18 50 C18 25 35 15 60 15 C85 15 102 25 102 50 C102 60 96 65 88 68 L32 68 C24 65 18 60 18 50 Z" fill="url(#hairGradient)"/>

                            <!-- Ears (partially hidden by hair) -->
                            <ellipse cx="26" cy="82" rx="4" ry="7" fill="#f0d0b8"/>
                            <ellipse cx="94" cy="82" rx="4" ry="7" fill="#f0d0b8"/>

                            <!-- Neck - quick feminine flare to shoulders -->
                            <path d="M48 125 C46 128 44 132 42 136 L78 136 C76 132 74 128 72 125" fill="url(#faceGradient)"/>
                            <!-- Neck hollow at base -->
                            <path d="M55 130 Q58 133 60 135 Q62 133 65 130" stroke="#d4a888" stroke-width="1.2" fill="none" opacity="0.35"/>

                            <!-- Face - more defined contour with stronger jawline -->
                            <path d="M25 80 Q25 55 60 52 Q95 55 95 80 Q95 100 82 115 Q70 128 60 130 Q50 128 38 115 Q25 100 25 80 Z" fill="url(#faceGradient)" id="faceShape"/>

                            <!-- Cheekbone highlights -->
                            <path d="M28 85 Q32 78 38 76" stroke="#fce8d8" stroke-width="2" fill="none" opacity="0.3" stroke-linecap="round"/>
                            <path d="M92 85 Q88 78 82 76" stroke="#fce8d8" stroke-width="2" fill="none" opacity="0.3" stroke-linecap="round"/>

                            <!-- Jaw line shadow - stronger definition -->
                            <path d="M28 92 Q32 108 42 118 Q48 124 55 127" stroke="#d0a080" stroke-width="1.5" fill="none" opacity="0.4"/>
                            <path d="M92 92 Q88 108 78 118 Q72 124 65 127" stroke="#d0a080" stroke-width="1.5" fill="none" opacity="0.4"/>

                            <!-- Chin definition -->
                            <path d="M50 125 Q55 129 60 130 Q65 129 70 125" stroke="#dbb8a0" stroke-width="1.2" fill="none" opacity="0.5"/>
                            <!-- Chin highlight -->
                            <ellipse cx="60" cy="124" rx="6" ry="3" fill="#fce8d8" opacity="0.15"/>

                            <!-- Temple shadows -->
                            <path d="M28 65 Q30 72 32 78" stroke="#dbb8a0" stroke-width="1" fill="none" opacity="0.3"/>
                            <path d="M92 65 Q90 72 88 78" stroke="#dbb8a0" stroke-width="1" fill="none" opacity="0.3"/>

                            <!-- Hair - flowing, connected, realistic -->
                            <!-- Main bangs/fringe - side swept -->
                            <path d="M25 60 C30 40 45 28 60 26 C75 28 90 40 95 60 C90 52 78 46 60 45 C42 46 30 52 25 60 Z" fill="url(#hairGradient)"/>
                            <!-- Left side hair - flows down connected to bangs -->
                            <path d="M25 60 C22 68 20 80 22 95 C24 110 20 130 18 150 C16 130 18 105 20 85 C21 72 23 64 25 60 Z" fill="url(#hairGradient)"/>
                            <!-- Right side hair - flows down connected to bangs -->
                            <path d="M95 60 C98 68 100 80 98 95 C96 110 100 130 102 150 C104 130 102 105 100 85 C99 72 97 64 95 60 Z" fill="url(#hairGradient)"/>
                            <!-- Hair highlight strands -->
                            <path d="M28 62 C26 75 25 95 24 115 C23 130 22 142 20 150" stroke="#6b4c35" stroke-width="1.5" fill="none" opacity="0.5"/>
                            <path d="M32 58 C30 72 28 92 27 112" stroke="#7a5a42" stroke-width="1" fill="none" opacity="0.4"/>
                            <path d="M92 62 C94 75 95 95 96 115 C97 130 98 142 100 150" stroke="#6b4c35" stroke-width="1.5" fill="none" opacity="0.5"/>
                            <path d="M88 58 C90 72 92 92 93 112" stroke="#7a5a42" stroke-width="1" fill="none" opacity="0.4"/>
                            <!-- Wispy strands near face -->
                            <path d="M30 65 C28 72 27 80 28 88" stroke="#5a3d2b" stroke-width="0.6" fill="none" opacity="0.4"/>
                            <path d="M90 65 C92 72 93 80 92 88" stroke="#5a3d2b" stroke-width="0.6" fill="none" opacity="0.4"/>

                            <!-- Eyebrows - longer, thicker, realistic with individual hair strokes -->
                            <g id="leftBrow">
                                <!-- Base brow shape - longer and thicker -->
                                <path d="M30 70 Q35 66 42 64.5 Q49 65 54 68" stroke="#4a3c30" stroke-width="2" fill="none" stroke-linecap="round"/>
                                <!-- Secondary thickness layer -->
                                <path d="M31 69 Q36 65.5 42 64 Q48 65 53 67.5" stroke="#3d3025" stroke-width="1.2" fill="none" opacity="0.6"/>
                                <!-- Individual hairs for texture -->
                                <path d="M32 70 Q33 67 34 66" stroke="#5d4e42" stroke-width="0.8" fill="none" opacity="0.8"/>
                                <path d="M36 68 Q37 65 38 64.5" stroke="#4a3c30" stroke-width="0.6" fill="none" opacity="0.7"/>
                                <path d="M40 66.5 Q41 65 42 64.5" stroke="#5d4e42" stroke-width="0.7" fill="none" opacity="0.8"/>
                                <path d="M44 66 Q45 65 46 64.5" stroke="#4a3c30" stroke-width="0.6" fill="none" opacity="0.7"/>
                                <path d="M48 66 Q49 65.5 50 66" stroke="#5d4e42" stroke-width="0.6" fill="none" opacity="0.7"/>
                                <path d="M51 67 Q52 66.5 53 67" stroke="#4a3c30" stroke-width="0.5" fill="none" opacity="0.6"/>
                            </g>
                            <g id="rightBrow">
                                <!-- Base brow shape - longer and thicker -->
                                <path d="M66 68 Q71 65 78 64.5 Q85 66 90 70" stroke="#4a3c30" stroke-width="2" fill="none" stroke-linecap="round"/>
                                <!-- Secondary thickness layer -->
                                <path d="M67 67.5 Q72 65 78 64 Q84 65.5 89 69" stroke="#3d3025" stroke-width="1.2" fill="none" opacity="0.6"/>
                                <!-- Individual hairs for texture -->
                                <path d="M67 67 Q68 66.5 69 67" stroke="#4a3c30" stroke-width="0.5" fill="none" opacity="0.6"/>
                                <path d="M70 66 Q71 65.5 72 66" stroke="#5d4e42" stroke-width="0.6" fill="none" opacity="0.7"/>
                                <path d="M74 66 Q75 65 76 64.5" stroke="#4a3c30" stroke-width="0.6" fill="none" opacity="0.7"/>
                                <path d="M78 66.5 Q79 65 80 64.5" stroke="#5d4e42" stroke-width="0.7" fill="none" opacity="0.8"/>
                                <path d="M82 68 Q83 65 84 64.5" stroke="#4a3c30" stroke-width="0.6" fill="none" opacity="0.7"/>
                                <path d="M86 70 Q87 67 88 66" stroke="#5d4e42" stroke-width="0.8" fill="none" opacity="0.8"/>
                            </g>

                            <!-- Eyes - almond shaped with whites -->
                            <ellipse cx="42" cy="78" rx="10" ry="6" fill="#fff" id="leftEyeWhite"/>
                            <ellipse cx="78" cy="78" rx="10" ry="6" fill="#fff" id="rightEyeWhite"/>

                            <!-- Iris with gradient -->
                            <circle cx="42" cy="78" r="4.5" fill="url(#irisGradient)" id="leftIris"/>
                            <circle cx="78" cy="78" r="4.5" fill="url(#irisGradient)" id="rightIris"/>

                            <!-- Pupils -->
                            <circle cx="42" cy="78" r="2.2" fill="#1a1a1a" id="leftPupil"/>
                            <circle cx="78" cy="78" r="2.2" fill="#1a1a1a" id="rightPupil"/>

                            <!-- Eye highlights -->
                            <circle cx="44" cy="76" r="1.3" fill="#fff" opacity="0.9"/>
                            <circle cx="80" cy="76" r="1.3" fill="#fff" opacity="0.9"/>

                            <!-- Upper eyelids - for blinking animation -->
                            <ellipse cx="42" cy="78" rx="10" ry="0" fill="url(#faceGradient)" id="leftEyelid"/>
                            <ellipse cx="78" cy="78" rx="10" ry="0" fill="url(#faceGradient)" id="rightEyelid"/>

                            <!-- Upper eyelid line/lash line -->
                            <path d="M32 74 Q37 71 42 70.5 Q47 71 52 74" stroke="#3d2e22" stroke-width="1.2" fill="none" stroke-linecap="round" id="leftLashLine"/>
                            <path d="M68 74 Q73 71 78 70.5 Q83 71 88 74" stroke="#3d2e22" stroke-width="1.2" fill="none" stroke-linecap="round" id="rightLashLine"/>

                            <!-- Lower lash line - subtle -->
                            <path d="M34 82 Q38 83.5 42 84 Q46 83.5 50 82" stroke="#6d5e52" stroke-width="0.6" fill="none" opacity="0.4"/>
                            <path d="M70 82 Q74 83.5 78 84 Q82 83.5 86 82" stroke="#6d5e52" stroke-width="0.6" fill="none" opacity="0.4"/>

                            <!-- Nose - subtle, natural -->
                            <path d="M60 78 L60 92" stroke="#dbb8a0" stroke-width="1.5" fill="none" stroke-linecap="round"/>
                            <path d="M55 95 Q58 97 60 96 Q62 97 65 95" stroke="#dbb8a0" stroke-width="1.2" fill="none" stroke-linecap="round"/>

                            <!-- Cheek blush - subtle -->
                            <ellipse cx="32" cy="92" rx="8" ry="4" fill="#f0a090" opacity="0.15"/>
                            <ellipse cx="88" cy="92" rx="8" ry="4" fill="#f0a090" opacity="0.15"/>

                            <!-- Lips - closed position (visible when not speaking) -->
                            <g id="closedLips">
                                <!-- Upper lip with cupid's bow -->
                                <path d="M48 106 Q52 104 56 105 L60 103 L64 105 Q68 104 72 106 Q68 108 60 108 Q52 108 48 106 Z" fill="url(#lipGradient)"/>
                                <!-- Lower lip -->
                                <path d="M48 106 Q52 108 60 108 Q68 108 72 106 Q70 110 60 111 Q50 110 48 106 Z" fill="#c4736a"/>
                            </g>

                            <!-- Open mouth group (hidden by default) - lips stay connected at corners -->
                            <g id="openMouth" opacity="0">
                                <!-- Mouth interior/darkness - smaller opening -->
                                <path d="M50 106 Q55 105 60 105 Q65 105 70 106 Q68 112 60 113 Q52 112 50 106 Z" fill="#3a1818" id="mouthInterior"/>

                                <!-- Upper teeth - subtle curved row -->
                                <path d="M52 106 Q56 105.5 60 105.5 Q64 105.5 68 106 L67 108.5 Q63 108 60 108 Q57 108 53 108.5 Z" fill="#f5f5ec" id="teeth" opacity="0.8"/>

                                <!-- Tongue - only visible when wide open -->
                                <ellipse cx="60" cy="111" rx="5" ry="2" fill="#c06060" id="tongue" opacity="0"/>

                                <!-- Upper lip - stays mostly in place -->
                                <path d="M48 106 Q52 104 56 105 L60 103 L64 105 Q68 104 72 106 Q68 106 60 105.5 Q52 106 48 106 Z" fill="url(#lipGradient)" id="upperLipOpen"/>

                                <!-- Lower lip - connected at corners (48 and 72), center drops -->
                                <path d="M48 106 Q52 110 60 112 Q68 110 72 106 Q70 111 60 113 Q50 111 48 106 Z" fill="#c4736a" id="lowerLipOpen"/>
                            </g>

                            <!-- Thinking smile - coy asymmetric side-smile -->
                            <g id="thinkingSmile" opacity="0">
                                <!-- Upper lip - slightly raised on right side -->
                                <path d="M48 107 Q54 105 60 104.5 Q66 104 74 102 Q70 106 62 107 Q54 107.5 48 107 Z" fill="url(#lipGradient)"/>
                                <!-- Lower lip - follows the asymmetric smile -->
                                <path d="M48 107 Q54 107.5 62 107 Q70 106 74 102 Q72 106 64 108.5 Q54 109 48 107 Z" fill="#c4736a"/>
                            </g>

                        </g>
                    </svg>
                </div>

                <div id="response" class="response-box">Waiting for input...</div>
                <div id="status" class="status-bar">Ready</div>

                <!-- TTS Panel -->
                <div class="tts-panel">
                    <h3>Text-to-Speech</h3>
                    <div class="tts-controls">
                        <input type="checkbox" id="ttsEnabled" checked>
                        <label for="ttsEnabled">Enable TTS</label>

                        <input type="checkbox" id="ttsStreaming" checked>
                        <label for="ttsStreaming">Stream (speak while generating)</label>

                        <button id="stopTtsBtn" class="btn-secondary" style="flex: 0; padding: 8px 16px;">Stop</button>
                        <button id="unlockAudioBtn" class="btn-success" style="flex: 0; padding: 8px 16px;">Unlock Audio</button>
                        <button id="testAudioBtn" class="btn-primary" style="flex: 0; padding: 8px 16px;">Test TTS</button>
                    </div>
                    <div class="tts-status" id="ttsStatus">TTS ready</div>
                    <div id="audioUnlockStatus" style="font-size: 0.8rem; color: #888; margin-top: 5px;"></div>
                    <div class="audio-queue" id="audioQueue"></div>
                </div>

                <!-- Debug Stats Panel -->
                <div class="debug-panel">
                    <h3>Performance Stats</h3>
                    <div class="stats-grid">
                        <div class="stat-item">
                            <div class="stat-label">Time to First Token</div>
                            <div class="stat-value" id="statTTFT">--</div>
                        </div>
                        <div class="stat-item">
                            <div class="stat-label">Total Latency</div>
                            <div class="stat-value" id="statLatency">--</div>
                        </div>
                        <div class="stat-item">
                            <div class="stat-label">Tokens/Second</div>
                            <div class="stat-value" id="statTPS">--</div>
                        </div>
                        <div class="stat-item">
                            <div class="stat-label">TTS Latency</div>
                            <div class="stat-value" id="statTTSLatency">--</div>
                        </div>
                        <div class="stat-item">
                            <div class="stat-label">Sentences Spoken</div>
                            <div class="stat-value highlight" id="statSentences">0</div>
                        </div>
                        <div class="stat-item">
                            <div class="stat-label">Request Count</div>
                            <div class="stat-value highlight" id="statRequests">0</div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </div>

    <script>
        // DOM Elements
        const webcam = document.getElementById('webcam');
        const canvas = document.getElementById('canvas');
        const snapshot = document.getElementById('snapshot');
        const promptInput = document.getElementById('prompt');
        const responseBox = document.getElementById('response');
        const statusBar = document.getElementById('status');
        const sendBtn = document.getElementById('sendBtn');
        const autosendBtn = document.getElementById('autosendBtn');
        const intervalInput = document.getElementById('interval');
        const apiUrlInput = document.getElementById('apiUrl');
        const ttsUrlInput = document.getElementById('ttsUrl');
        const modelNameInput = document.getElementById('modelName');
        const resolutionSelect = document.getElementById('resolution');
        const resolutionInfo = document.getElementById('resolutionInfo');
        const cameraSelect = document.getElementById('cameraSelect');

        // TTS elements
        const ttsEnabled = document.getElementById('ttsEnabled');
        const ttsStreaming = document.getElementById('ttsStreaming');
        const ttsStatus = document.getElementById('ttsStatus');
        const audioQueue = document.getElementById('audioQueue');
        const stopTtsBtn = document.getElementById('stopTtsBtn');
        const unlockAudioBtn = document.getElementById('unlockAudioBtn');
        const testAudioBtn = document.getElementById('testAudioBtn');
        const audioUnlockStatus = document.getElementById('audioUnlockStatus');

        // Stats elements
        const statTTFT = document.getElementById('statTTFT');
        const statLatency = document.getElementById('statLatency');
        const statTPS = document.getElementById('statTPS');
        const statTTSLatency = document.getElementById('statTTSLatency');
        const statSentences = document.getElementById('statSentences');
        const statRequests = document.getElementById('statRequests');

        // State
        let stream = null;
        let autosendInterval = null;
        let isAutosending = false;
        let isProcessing = false;
        let requestCount = 0;
        let sentenceCount = 0;
        let currentResolution = { width: 640, height: 480 };
        let selectedDeviceId = null;

        // TTS State
        let ttsAudioQueue = [];
        let isPlayingAudio = false;
        let currentAudio = null;
        let pendingSentence = '';
        let failedAudioItem = null; // Store failed audio for retry after unlock
        let ttsSequenceNumber = 0; // Monotonic sequence for ordering
        let nextExpectedSequence = 0; // Next sequence number to play
        let pendingAudioItems = new Map(); // Out-of-order items waiting to be queued

        // Avatar animation state
        let audioContext = null;
        let analyser = null;
        let animationFrameId = null;
        let blinkAnimationId = null;
        let idleAnimationId = null;
        let nextBlinkTime = 0;
        let isBlinking = false;
        let blinkProgress = 0;
        let isThinking = false;
        const headGroup = document.getElementById('headGroup');
        const closedLips = document.getElementById('closedLips');
        const openMouth = document.getElementById('openMouth');
        const thinkingSmile = document.getElementById('thinkingSmile');
        const mouthInterior = document.getElementById('mouthInterior');
        const teeth = document.getElementById('teeth');
        const tongue = document.getElementById('tongue');
        const lowerLipOpen = document.getElementById('lowerLipOpen');
        const leftPupil = document.getElementById('leftPupil');
        const rightPupil = document.getElementById('rightPupil');
        const leftIris = document.getElementById('leftIris');
        const rightIris = document.getElementById('rightIris');
        const leftEyelid = document.getElementById('leftEyelid');
        const rightEyelid = document.getElementById('rightEyelid');
        const leftEyeWhite = document.getElementById('leftEyeWhite');
        const rightEyeWhite = document.getElementById('rightEyeWhite');

        // Resolution presets
        const resolutionPresets = {
            '320x240': { width: 320, height: 240, estKB: 15 },
            '640x480': { width: 640, height: 480, estKB: 50 },
            '800x600': { width: 800, height: 600, estKB: 80 },
            '1280x720': { width: 1280, height: 720, estKB: 150 },
            '1920x1080': { width: 1920, height: 1080, estKB: 300 }
        };

        // ==================== Avatar Animation ====================

        // Track audio sources to avoid re-creating them
        let audioSourceMap = new WeakMap();
        let audioContextResumed = false;

        // Initialize audio context for analyzing playback
        function initAudioContext() {
            if (!audioContext) {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                analyser = audioContext.createAnalyser();
                analyser.fftSize = 256;
                analyser.smoothingTimeConstant = 0.7;
            }
        }

        // Resume audio context (required on mobile after user interaction)
        async function resumeAudioContext() {
            if (audioContext && audioContext.state === 'suspended') {
                try {
                    await audioContext.resume();
                    audioContextResumed = true;
                    console.log('AudioContext resumed');
                } catch (e) {
                    console.error('Failed to resume AudioContext:', e);
                }
            }
        }

        // Explicit audio unlock for mobile browsers
        async function unlockAudio() {
            try {
                audioUnlockStatus.textContent = 'Unlocking audio...';
                audioUnlockStatus.style.color = '#ffc107';

                // Initialize audio context
                initAudioContext();

                // Resume if suspended
                if (audioContext.state === 'suspended') {
                    await audioContext.resume();
                }

                // Play a silent buffer to fully unlock audio
                const buffer = audioContext.createBuffer(1, 1, 22050);
                const source = audioContext.createBufferSource();
                source.buffer = buffer;
                source.connect(audioContext.destination);
                source.start(0);

                // Create an oscillator and play it briefly (works better on Chrome)
                const oscillator = audioContext.createOscillator();
                const gainNode = audioContext.createGain();
                gainNode.gain.value = 0.001; // Nearly silent
                oscillator.connect(gainNode);
                gainNode.connect(audioContext.destination);
                oscillator.start(0);
                oscillator.stop(audioContext.currentTime + 0.1);

                // Also try playing a real Audio element with user interaction
                // Use a longer silent WAV that Chrome accepts better
                const silentWav = 'data:audio/wav;base64,UklGRnoGAABXQVZFZm10IBAAAAABAAEAQB8AAEAfAAABAAgAZGF0YQoGAACBhYqFbF1fdJivrJBhNjVgodDbq2EcBj+a2teleRcAGo3V5Z+HOwwgjNHqpIxDEgUvms/soZFKGQk1oc7qoZVPHg09ps3onZhTIxNFq8zklJlXJxZLrsnjj5lbKxlQsMfggJldLxxVssXdd5xfMCBZtMPZcJ1hMyNct8LVaJ5jNiZguMHSYZ5lOSljusDPWp9nPCxmvL/NVKBpPy9ov76LUaFrQjJrwL2HT6JtRTVtwbyDTKNvSDhvw7t/SaRxSjtyxLp7RqsziWPZn2EcDjqV2eWdg0AWFDmY2OKVgEYVGT+b1+CSgEsZHz+e1N2PgE8cIkKh0tqNgFMfJUSj0NeLgFciKEem0NSIgFolK0mo0NGGgF4oLkuqz86DgGErMU6sz8yBgGQuNFCu0MuAYGcxNFGv0MuAYGg0NlSx0MyAYGo3OFay0cyAYGw5OVi00s2AYG07PFq10s2AYG8+Pl290s6AYHFBQGDh4dHPm3xbJxlMq8XTeZdcMCFXtcDJX5JePihecL7HXI9gQSlibrvFWYxiRCxlcLnDVotkRy9ocbfBU4hmSjJrc7W/UYZoTTVudLO9ToRqUDhweLC7TIJsUzt0e66ySn1uVj14f6yzSH1wWUB8g6qwRntzXER/hqiuRHl1X0eCiaasPHZ3Ykl/i6SrOnR5ZU2Cjaqqe5F2XUSAipumjY51WkB8hZSifHxzcnh3hpOhem1xb3JzeZCgd2dsbW1udY6edmRpa2pscI2cc2FmaGdpa4uabF5jZWRmaImYaVtgY2FjZYaVZlhdYF5gYoOTY1VaXltdX4CQYFJYXFlbXX2OXVBVW1dYW3qLWk1TWVRWWHeIV0pQV1FTVU5GQj1AOTo7Ozk4NjY1RERPT1NSWV9ka3F4f4aNlJuiqq+1u8HGy9DV2t3g4+Xo6uvs7e3t7u7u7u7u7u7u7e3s7Ovq6Obl4+Df3NrX1NHOysfDv7u3sq6ppqKenZycm5qamZiYl5eWlpaVlZWUlJSTk5OSkpKRkZCQkI+Pjo6NjYyMi4uKioiIh4eGhoWFhISCgoGBgH9+fXx7enl4d3Z1dHNycXBvbm1sa2ppaGdmZWRjYWBfXlxbWllYV1VUUU9OTEtJSEZFQ0JAQD49OzQdBz5rn8rXrmMRBDiS0N6igj8OCjCP0eGdgkQQCjWQz+KehEUSCDmSzuGahkcTBzuVzeGYhkkVBj6Yy+GViEsVBkGay+GUiU0WBkOcyuCTik8XBEWeyuCQi1EYBEifyuCPjFMZA0mhy9+OjVUZA0uj';
                const silentAudio = new Audio(silentWav);
                silentAudio.volume = 0.01;

                // Wait for the audio to be playable
                await new Promise((resolve, reject) => {
                    silentAudio.oncanplaythrough = resolve;
                    silentAudio.onerror = reject;
                    setTimeout(resolve, 1000); // Timeout fallback
                });

                await silentAudio.play();

                // Let it play briefly then pause
                await new Promise(r => setTimeout(r, 100));
                silentAudio.pause();

                audioContextResumed = true;

                // Check if there's failed audio to replay
                if (failedAudioItem) {
                    audioUnlockStatus.textContent = 'Playing audio...';
                    audioUnlockStatus.style.color = '#00c853';
                    unlockAudioBtn.textContent = 'Audio Unlocked';
                    unlockAudioBtn.disabled = true;
                    unlockAudioBtn.classList.remove('btn-success');
                    unlockAudioBtn.classList.add('btn-secondary');

                    // Put failed item back at front of queue and play
                    ttsAudioQueue.unshift(failedAudioItem);
                    failedAudioItem = null;
                    console.log('Replaying failed audio after unlock');
                    playNextAudio();
                } else {
                    audioUnlockStatus.textContent = 'Audio unlocked! Ready to play.';
                    audioUnlockStatus.style.color = '#00c853';
                    unlockAudioBtn.textContent = 'Audio Unlocked';
                    unlockAudioBtn.disabled = true;
                    unlockAudioBtn.classList.remove('btn-success');
                    unlockAudioBtn.classList.add('btn-secondary');
                }

                console.log('Audio unlocked successfully, AudioContext state:', audioContext.state);
                return true;
            } catch (e) {
                console.error('Failed to unlock audio:', e);
                audioUnlockStatus.textContent = 'Failed: ' + e.message + '. Try tapping Send.';
                audioUnlockStatus.style.color = '#e94560';
                return false;
            }
        }

        // Check if audio needs unlocking (detect mobile)
        function checkAudioStatus() {
            const isMobile = /iPhone|iPad|iPod|Android/i.test(navigator.userAgent);
            if (isMobile) {
                audioUnlockStatus.textContent = 'Tap "Unlock Audio" then "Test TTS" to enable speech';
                audioUnlockStatus.style.color = '#ffc107';
            } else {
                // On desktop, try to unlock automatically
                initAudioContext();
                if (audioContext.state === 'running') {
                    audioUnlockStatus.textContent = 'Audio ready';
                    audioUnlockStatus.style.color = '#00c853';
                }
            }
        }

        // Test TTS by fetching and playing real audio
        async function testTTSAudio() {
            try {
                audioUnlockStatus.textContent = 'Testing TTS...';
                audioUnlockStatus.style.color = '#ffc107';

                // First unlock audio
                await unlockAudio();

                // Fetch TTS audio
                const ttsUrl = ttsUrlInput.value.replace(/\/$/, '') + '/api/tts';
                const response = await fetch(ttsUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ text: 'Hello! Audio is working.' })
                });

                if (!response.ok) {
                    throw new Error('TTS request failed: ' + response.status);
                }

                const audioBlob = await response.blob();
                const audioUrl = URL.createObjectURL(audioBlob);

                // Play the audio
                const testAudio = new Audio();
                testAudio.src = audioUrl;
                testAudio.volume = 1.0;
                testAudio.playsInline = true;
                testAudio.setAttribute('playsinline', '');

                testAudio.onended = () => {
                    URL.revokeObjectURL(audioUrl);
                    audioUnlockStatus.textContent = 'Audio test successful!';
                    audioUnlockStatus.style.color = '#00c853';
                };

                testAudio.onerror = (e) => {
                    console.error('Test audio error:', e);
                    audioUnlockStatus.textContent = 'Audio element error';
                    audioUnlockStatus.style.color = '#e94560';
                };

                testAudio.load();
                await testAudio.play();

                audioUnlockStatus.textContent = 'Playing test audio...';
                audioUnlockStatus.style.color = '#00c853';

            } catch (e) {
                console.error('Test TTS failed:', e);
                audioUnlockStatus.textContent = 'Test failed: ' + e.message;
                audioUnlockStatus.style.color = '#e94560';
            }
        }

        // Blinking animation - runs continuously
        function animateBlink() {
            const now = Date.now();

            // Skip blinking during thinking (eyes are already narrowed)
            if (isThinking) {
                blinkAnimationId = requestAnimationFrame(animateBlink);
                return;
            }

            // Schedule next blink with random variation (2-6 seconds)
            if (now >= nextBlinkTime && !isBlinking) {
                isBlinking = true;
                blinkProgress = 0;
                // Occasional double blink (20% chance)
                const isDoubleBlink = Math.random() < 0.2;
                animateBlinkCycle(isDoubleBlink);
            }

            blinkAnimationId = requestAnimationFrame(animateBlink);
        }

        // Animate a single blink cycle
        function animateBlinkCycle(doubleBlink = false) {
            const blinkDuration = 120; // ms for full close
            const holdDuration = 40;   // ms eyes closed
            const startTime = Date.now();

            function blinkFrame() {
                const elapsed = Date.now() - startTime;
                const totalCycle = blinkDuration * 2 + holdDuration;

                let eyelidRy;
                if (elapsed < blinkDuration) {
                    // Closing - ease in
                    const t = elapsed / blinkDuration;
                    eyelidRy = 6 * (t * t); // Quadratic ease in
                } else if (elapsed < blinkDuration + holdDuration) {
                    // Holding closed
                    eyelidRy = 6;
                } else if (elapsed < totalCycle) {
                    // Opening - ease out
                    const t = (elapsed - blinkDuration - holdDuration) / blinkDuration;
                    eyelidRy = 6 * (1 - t * t); // Quadratic ease out
                } else {
                    // Blink complete
                    eyelidRy = 0;
                    leftEyelid.setAttribute('ry', '0');
                    rightEyelid.setAttribute('ry', '0');

                    if (doubleBlink) {
                        // Do second blink after short pause
                        setTimeout(() => animateBlinkCycle(false), 80);
                    } else {
                        isBlinking = false;
                        // Schedule next blink: 2-6 seconds with variation
                        const baseInterval = 3000 + Math.random() * 3000;
                        // Add some irregularity
                        const variation = (Math.random() - 0.5) * 1000;
                        nextBlinkTime = Date.now() + baseInterval + variation;
                    }
                    return;
                }

                leftEyelid.setAttribute('ry', eyelidRy.toFixed(1));
                rightEyelid.setAttribute('ry', eyelidRy.toFixed(1));

                requestAnimationFrame(blinkFrame);
            }

            blinkFrame();
        }

        // Start blink animation loop
        function startBlinkAnimation() {
            if (!blinkAnimationId) {
                nextBlinkTime = Date.now() + 1000 + Math.random() * 2000; // First blink in 1-3 sec
                animateBlink();
            }
        }

        // Stop blink animation
        function stopBlinkAnimation() {
            if (blinkAnimationId) {
                cancelAnimationFrame(blinkAnimationId);
                blinkAnimationId = null;
            }
            // Reset eyelids to open
            leftEyelid.setAttribute('ry', '0');
            rightEyelid.setAttribute('ry', '0');
            isBlinking = false;
        }

        // Idle animation - subtle head movement when not speaking or thinking
        function animateIdle() {
            if (isPlayingAudio || isThinking) {
                idleAnimationId = requestAnimationFrame(animateIdle);
                return;
            }

            const time = Date.now() / 1000;

            // Very subtle head movement - gentle swaying
            const headX = Math.sin(time * 0.3) * 0.8 + Math.sin(time * 0.7) * 0.4;
            const headY = Math.cos(time * 0.25) * 0.5 + Math.cos(time * 0.6) * 0.3;
            const headRotate = Math.sin(time * 0.2) * 1.2;
            headGroup.setAttribute('transform', `translate(${headX}, ${headY}) rotate(${headRotate}, 60, 90)`);

            // Subtle eye movement - occasional glances
            const eyeX = Math.sin(time * 0.5) * 1.5 + Math.sin(time * 1.3) * 0.5;
            const eyeY = Math.cos(time * 0.4) * 0.8 + Math.cos(time * 1.1) * 0.3;

            leftIris.setAttribute('cx', 42 + eyeX);
            leftIris.setAttribute('cy', 78 + eyeY);
            leftPupil.setAttribute('cx', 42 + eyeX);
            leftPupil.setAttribute('cy', 78 + eyeY);
            rightIris.setAttribute('cx', 78 + eyeX);
            rightIris.setAttribute('cy', 78 + eyeY);
            rightPupil.setAttribute('cx', 78 + eyeX);
            rightPupil.setAttribute('cy', 78 + eyeY);

            idleAnimationId = requestAnimationFrame(animateIdle);
        }

        // Start idle animation
        function startIdleAnimation() {
            if (!idleAnimationId) {
                animateIdle();
            }
        }

        // Stop idle animation
        function stopIdleAnimation() {
            if (idleAnimationId) {
                cancelAnimationFrame(idleAnimationId);
                idleAnimationId = null;
            }
        }

        // Thinking animation - coy side-smile with more narrowed eyes
        function startThinkingAnimation() {
            isThinking = true;
            // Show coy side-smile, hide normal lips
            closedLips.setAttribute('opacity', '0');
            thinkingSmile.setAttribute('opacity', '1');
            // Narrow the eye whites more (ry from 6 to 4)
            leftEyeWhite.setAttribute('ry', '4');
            rightEyeWhite.setAttribute('ry', '4');
        }

        function animateThinking() {
            if (!isThinking) return;

            const time = Date.now() / 1000;

            // Very subtle head movement
            const headX = Math.sin(time * 0.25) * 0.6;
            const headY = Math.cos(time * 0.2) * 0.4;
            const headRotate = Math.sin(time * 0.18) * 1.2;
            headGroup.setAttribute('transform', `translate(${headX}, ${headY}) rotate(${headRotate}, 60, 90)`);

            // Eyes looking slightly up and to the side - contemplative
            const eyeX = 1.2 + Math.sin(time * 0.35) * 0.6;
            const eyeY = -1 + Math.sin(time * 0.4) * 0.3;

            leftIris.setAttribute('cx', 42 + eyeX);
            leftIris.setAttribute('cy', 78 + eyeY);
            leftPupil.setAttribute('cx', 42 + eyeX);
            leftPupil.setAttribute('cy', 78 + eyeY);
            rightIris.setAttribute('cx', 78 + eyeX);
            rightIris.setAttribute('cy', 78 + eyeY);
            rightPupil.setAttribute('cx', 78 + eyeX);
            rightPupil.setAttribute('cy', 78 + eyeY);

            // Very subtle eye narrowing variation (base 4, slight variation)
            const eyeNarrow = 4 + Math.sin(time * 0.5) * 0.2;
            leftEyeWhite.setAttribute('ry', eyeNarrow.toFixed(1));
            rightEyeWhite.setAttribute('ry', eyeNarrow.toFixed(1));

            if (isThinking) {
                requestAnimationFrame(animateThinking);
            }
        }

        function stopThinkingAnimation() {
            isThinking = false;
            // Reset lips
            thinkingSmile.setAttribute('opacity', '0');
            closedLips.setAttribute('opacity', '1');
            // Reset eye whites to normal
            leftEyeWhite.setAttribute('ry', '6');
            rightEyeWhite.setAttribute('ry', '6');
            // Reset head
            headGroup.setAttribute('transform', '');
            // Reset eyes to center
            leftIris.setAttribute('cx', '42');
            leftIris.setAttribute('cy', '78');
            leftPupil.setAttribute('cx', '42');
            leftPupil.setAttribute('cy', '78');
            rightIris.setAttribute('cx', '78');
            rightIris.setAttribute('cy', '78');
            rightPupil.setAttribute('cx', '78');
            rightPupil.setAttribute('cy', '78');
        }

        // Track whether to use analyser or fallback animation
        let useAudioAnalyser = true;

        // Animate avatar mouth based on audio amplitude
        function animateMouth() {
            const time = Date.now() / 1000;

            if (!isPlayingAudio) {
                // Reset to natural closed lips when not playing
                closedLips.setAttribute('opacity', '1');
                openMouth.setAttribute('opacity', '0');
                // Reset head position
                headGroup.setAttribute('transform', '');
                return;
            }

            let amplitude;

            if (useAudioAnalyser && analyser) {
                // Use audio analyser for amplitude
                const dataArray = new Uint8Array(analyser.frequencyBinCount);
                analyser.getByteFrequencyData(dataArray);

                // Calculate average amplitude (focus on speech frequencies)
                let sum = 0;
                const speechBins = Math.floor(analyser.frequencyBinCount * 0.4);
                for (let i = 0; i < speechBins; i++) {
                    sum += dataArray[i];
                }
                const average = sum / speechBins;

                // Normalize to 0-1 range - reduced for smaller mouth opening
                amplitude = Math.min(average / 110, 1);
            } else {
                // Fallback: simulate speech amplitude with varied sine waves
                // Creates a realistic speaking pattern
                amplitude = Math.abs(
                    Math.sin(time * 8) * 0.3 +
                    Math.sin(time * 12.5) * 0.25 +
                    Math.sin(time * 17) * 0.15 +
                    Math.sin(time * 23) * 0.1
                ) * 0.8;
                // Add pauses for more realistic speech
                const pauseFactor = Math.sin(time * 2) > 0.7 ? 0.2 : 1.0;
                amplitude *= pauseFactor;
            }

            // Subtle head movement while speaking
            const headTiltX = Math.sin(time * 0.7) * 1.5;
            const headTiltY = Math.cos(time * 0.5) * 0.8;
            const headRotate = Math.sin(time * 0.4) * 2;
            headGroup.setAttribute('transform', `translate(${headTiltX}, ${headTiltY}) rotate(${headRotate}, 60, 80)`);

            // Transition from closed lips to open mouth
            if (amplitude > 0.08) {
                closedLips.setAttribute('opacity', '0');
                openMouth.setAttribute('opacity', '1');

                // Smaller, more realistic mouth opening
                const openY = amplitude * 4; // Reduced from 6

                // Mouth interior - lips stay connected at corners (x=48 and x=72)
                const interiorPath = `M50 106 Q55 105 60 105 Q65 105 70 106 Q68 ${109 + openY * 0.6} 60 ${110 + openY} Q52 ${109 + openY * 0.6} 50 106 Z`;
                mouthInterior.setAttribute('d', interiorPath);

                // Lower lip - CONNECTED at corners, only center drops
                // Corners stay at y=106 (matching upper lip corners at 48,106 and 72,106)
                const centerDrop = openY * 0.9;
                const lowerLipPath = `M48 106 Q52 ${108 + centerDrop * 0.5} 60 ${109 + centerDrop} Q68 ${108 + centerDrop * 0.5} 72 106 Q70 ${108 + centerDrop * 0.4} 60 ${110 + openY} Q50 ${108 + centerDrop * 0.4} 48 106 Z`;
                lowerLipOpen.setAttribute('d', lowerLipPath);

                // Tongue - only visible when more open
                tongue.setAttribute('cy', 109 + openY * 0.5);
                tongue.setAttribute('ry', 1.5 + openY * 0.2);
                tongue.setAttribute('opacity', amplitude > 0.4 ? Math.min((amplitude - 0.4) * 1.5, 0.7).toFixed(2) : '0');

                // Teeth - subtle visibility
                teeth.setAttribute('opacity', amplitude > 0.15 ? Math.min(amplitude * 0.5, 0.5).toFixed(2) : '0');

            } else {
                closedLips.setAttribute('opacity', '1');
                openMouth.setAttribute('opacity', '0');
            }

            // Natural eye movement - look around, occasional glances
            const eyeBaseX = Math.sin(time * 0.8) * 2 + Math.sin(time * 2.1) * 0.5;
            const eyeBaseY = Math.cos(time * 0.6) * 1 + Math.cos(time * 1.7) * 0.3;

            leftIris.setAttribute('cx', 42 + eyeBaseX);
            leftIris.setAttribute('cy', 78 + eyeBaseY);
            leftPupil.setAttribute('cx', 42 + eyeBaseX);
            leftPupil.setAttribute('cy', 78 + eyeBaseY);

            rightIris.setAttribute('cx', 78 + eyeBaseX);
            rightIris.setAttribute('cy', 78 + eyeBaseY);
            rightPupil.setAttribute('cx', 78 + eyeBaseX);
            rightPupil.setAttribute('cy', 78 + eyeBaseY);

            animationFrameId = requestAnimationFrame(animateMouth);
        }

        // Start mouth animation loop
        function startMouthAnimation(withAnalyser = true) {
            useAudioAnalyser = withAnalyser;
            if (!animationFrameId) {
                animateMouth();
            }
        }

        // Stop mouth animation
        function stopMouthAnimation() {
            if (animationFrameId) {
                cancelAnimationFrame(animationFrameId);
                animationFrameId = null;
            }
            // Reset to natural closed lips
            closedLips.setAttribute('opacity', '1');
            openMouth.setAttribute('opacity', '0');
            // Reset eyes to center
            leftIris.setAttribute('cx', '42');
            leftIris.setAttribute('cy', '78');
            leftPupil.setAttribute('cx', '42');
            leftPupil.setAttribute('cy', '78');
            rightIris.setAttribute('cx', '78');
            rightIris.setAttribute('cy', '78');
            rightPupil.setAttribute('cx', '78');
            rightPupil.setAttribute('cy', '78');
        }

        // ==================== TTS Functions ====================

        // Sentence boundary detection for streaming TTS
        function extractCompleteSentences(text) {
            // Match sentences ending with . ! ? followed by space or end
            const sentenceRegex = /[^.!?]*[.!?]+(?:\s|$)/g;
            const sentences = [];
            let match;
            let lastIndex = 0;

            while ((match = sentenceRegex.exec(text)) !== null) {
                sentences.push(match[0].trim());
                lastIndex = sentenceRegex.lastIndex;
            }

            // Return sentences found and remaining text
            const remaining = text.slice(lastIndex);
            return { sentences, remaining };
        }

        // Queue audio for playback with sequence ordering
        function queueTTSAudio(audioBlob, sentenceText, sequenceNum) {
            const audioUrl = URL.createObjectURL(audioBlob);
            const item = { url: audioUrl, text: sentenceText, seq: sequenceNum };

            // If this is the next expected sequence, add to queue directly
            if (sequenceNum === nextExpectedSequence) {
                ttsAudioQueue.push(item);
                nextExpectedSequence++;

                // Check if we have any pending items that can now be added
                while (pendingAudioItems.has(nextExpectedSequence)) {
                    ttsAudioQueue.push(pendingAudioItems.get(nextExpectedSequence));
                    pendingAudioItems.delete(nextExpectedSequence);
                    nextExpectedSequence++;
                }

                updateAudioQueueDisplay();

                if (!isPlayingAudio) {
                    playNextAudio();
                }
            } else if (sequenceNum > nextExpectedSequence) {
                // Out of order - store for later
                console.log(`Audio seq ${sequenceNum} arrived out of order, expecting ${nextExpectedSequence}`);
                pendingAudioItems.set(sequenceNum, item);
            } else {
                // Old sequence number (shouldn't happen, but handle it)
                console.warn(`Ignoring old audio sequence ${sequenceNum}, already at ${nextExpectedSequence}`);
                URL.revokeObjectURL(audioUrl);
            }
        }

        // Play next audio in queue
        async function playNextAudio() {
            if (ttsAudioQueue.length === 0) {
                isPlayingAudio = false;
                stopMouthAnimation();
                setTTSStatus('TTS idle');
                return;
            }

            isPlayingAudio = true;
            const item = ttsAudioQueue.shift();
            updateAudioQueueDisplay();

            setTTSStatus(`Speaking: "${item.text.slice(0, 50)}${item.text.length > 50 ? '...' : ''}"`);

            // Create audio element - simple approach for maximum compatibility
            currentAudio = new Audio();
            currentAudio.src = item.url;
            currentAudio.volume = 1.0;
            currentAudio.playsInline = true; // Required for mobile Chrome
            currentAudio.setAttribute('playsinline', ''); // Also set as attribute

            // Try to use analyser for lip sync, but don't require it
            let useAnalyser = false;
            try {
                initAudioContext();
                await resumeAudioContext();

                // Only try MediaElementSource on desktop or if explicitly unlocked
                if (audioContextResumed && audioContext.state === 'running') {
                    // Check if we already created a source for this audio element
                    if (!audioSourceMap.has(currentAudio)) {
                        currentAudio.crossOrigin = 'anonymous';
                        const source = audioContext.createMediaElementSource(currentAudio);
                        source.connect(analyser);
                        analyser.connect(audioContext.destination);
                        audioSourceMap.set(currentAudio, source);
                    }
                    useAnalyser = true;
                }
            } catch (e) {
                console.warn('Audio analyser not available, using fallback animation:', e);
                useAnalyser = false;
            }

            // Start mouth animation (will use analyser if available, fallback otherwise)
            startMouthAnimation(useAnalyser);

            currentAudio.onended = () => {
                URL.revokeObjectURL(item.url);
                currentAudio = null;
                // Don't stop animation yet if more audio is queued
                if (ttsAudioQueue.length === 0) {
                    stopMouthAnimation();
                }
                playNextAudio();
            };

            currentAudio.onerror = (e) => {
                console.error('Audio playback error:', e);
                audioUnlockStatus.textContent = 'Audio error - try Unlock Audio button';
                audioUnlockStatus.style.color = '#e94560';
                URL.revokeObjectURL(item.url);
                currentAudio = null;
                stopMouthAnimation();
                playNextAudio();
            };

            try {
                // Use load() then play() for better mobile compatibility
                currentAudio.load();
                await currentAudio.play();
                // Clear any failed item on success
                failedAudioItem = null;
            } catch (err) {
                console.error('Failed to play audio:', err);
                setTTSStatus(`Audio error - tap Unlock Audio to retry`, true);

                // Save failed item for retry - put current item back at front of queue
                failedAudioItem = { url: item.url, text: item.text };

                audioUnlockStatus.textContent = 'Tap "Unlock Audio" to play speech';
                audioUnlockStatus.style.color = '#ffc107';

                // Re-enable unlock button
                unlockAudioBtn.disabled = false;
                unlockAudioBtn.textContent = 'Unlock Audio';
                unlockAudioBtn.classList.remove('btn-secondary');
                unlockAudioBtn.classList.add('btn-success');

                stopMouthAnimation();
                isPlayingAudio = false;
                // Don't call playNextAudio - wait for user to unlock
            }
        }

        // Stop all TTS playback
        function stopTTS() {
            if (currentAudio) {
                currentAudio.pause();
                currentAudio = null;
            }
            ttsAudioQueue.forEach(item => URL.revokeObjectURL(item.url));
            ttsAudioQueue = [];
            isPlayingAudio = false;
            pendingSentence = '';
            stopMouthAnimation();
            updateAudioQueueDisplay();
            setTTSStatus('TTS stopped');
        }

        // Update audio queue display
        function updateAudioQueueDisplay() {
            audioQueue.innerHTML = '';
            if (isPlayingAudio) {
                const playing = document.createElement('span');
                playing.className = 'audio-chunk playing';
                playing.textContent = 'â–¶ Playing';
                audioQueue.appendChild(playing);
            }
            ttsAudioQueue.forEach((item, i) => {
                const chunk = document.createElement('span');
                chunk.className = 'audio-chunk queued';
                chunk.textContent = `${i + 1}: ${item.text.slice(0, 20)}...`;
                audioQueue.appendChild(chunk);
            });
        }

        // Set TTS status
        function setTTSStatus(message, isError = false) {
            ttsStatus.textContent = message;
            ttsStatus.className = 'tts-status' + (isError ? ' error' : (isPlayingAudio ? ' speaking' : ''));
        }

        // Synthesize speech using Piper TTS API (via proxy)
        async function synthesizeSpeech(text) {
            if (!text.trim()) return null;

            const startTime = performance.now();

            try {
                // Use proxied TTS endpoint to avoid CORS issues
                // Server proxies /tts/* to http://localhost:7860/*
                const ttsUrl = ttsUrlInput.value;
                const apiUrl = ttsUrl.replace(/\/$/, '') + '/api/tts';

                const response = await fetch(apiUrl, {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    },
                    body: JSON.stringify({
                        text: text
                    }),
                });

                if (!response.ok) {
                    const errorText = await response.text();
                    throw new Error(`TTS API error: ${response.status} - ${errorText}`);
                }

                // Piper returns WAV audio directly
                const blob = await response.blob();

                const latency = performance.now() - startTime;
                statTTSLatency.textContent = `${latency.toFixed(0)} ms`;
                return blob;

            } catch (err) {
                console.error('TTS synthesis error:', err);
                setTTSStatus(`TTS error: ${err.message}`, true);
                return null;
            }
        }

        // Process text for TTS (streaming or batch)
        async function processTextForTTS(newContent, isComplete = false) {
            if (!ttsEnabled.checked) return;

            pendingSentence += newContent;

            if (ttsStreaming.checked) {
                // Streaming mode: synthesize complete sentences as they arrive
                const { sentences, remaining } = extractCompleteSentences(pendingSentence);
                pendingSentence = remaining;

                // Launch all TTS requests in parallel but with assigned sequence numbers
                const synthesisPromises = sentences
                    .filter(s => s.trim().length > 2)
                    .map(sentence => {
                        const seqNum = ttsSequenceNumber++;
                        sentenceCount++;
                        statSentences.textContent = sentenceCount.toString();
                        setTTSStatus(`Synthesizing: "${sentence.slice(0, 30)}..."`);

                        return synthesizeSpeech(sentence).then(audioBlob => {
                            if (audioBlob) {
                                queueTTSAudio(audioBlob, sentence, seqNum);
                            }
                        });
                    });

                // Wait for all to complete (but they'll be queued in order due to sequence numbers)
                await Promise.all(synthesisPromises);
            }

            // On completion, synthesize any remaining text
            if (isComplete && pendingSentence.trim().length > 2) {
                const seqNum = ttsSequenceNumber++;
                if (!ttsStreaming.checked) {
                    // Batch mode: synthesize all at once
                    setTTSStatus('Synthesizing full response...');
                    const audioBlob = await synthesizeSpeech(pendingSentence);
                    if (audioBlob) {
                        sentenceCount++;
                        statSentences.textContent = sentenceCount.toString();
                        queueTTSAudio(audioBlob, pendingSentence, seqNum);
                    }
                } else {
                    // Streaming mode: synthesize remaining fragment
                    sentenceCount++;
                    statSentences.textContent = sentenceCount.toString();
                    const audioBlob = await synthesizeSpeech(pendingSentence);
                    if (audioBlob) {
                        queueTTSAudio(audioBlob, pendingSentence, seqNum);
                    }
                }
                pendingSentence = '';
            }
        }

        // ==================== Camera Functions ====================

        function checkSecureContext() {
            if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                const isSecure = window.isSecureContext;
                const protocol = window.location.protocol;
                const hostname = window.location.hostname;

                let msg = 'Camera API not available. ';
                if (!isSecure) {
                    msg += `Requires HTTPS or localhost. Current: ${protocol}//${hostname}`;
                }

                console.error(msg);
                cameraSelect.innerHTML = '<option value="">Camera requires HTTPS</option>';
                setStatus(msg, 'error');

                responseBox.innerHTML = `<strong>Camera Access Blocked</strong>\n\n` +
                    `Browsers require HTTPS for camera access (except localhost).\n\n` +
                    `<strong>Options:</strong>\n` +
                    `1. Access from the Jetson directly: http://localhost:8443\n` +
                    `2. Use Chrome with security flag\n` +
                    `3. Accept the self-signed certificate\n\n` +
                    `Current URL: ${window.location.href}\n` +
                    `Secure context: ${isSecure ? 'Yes' : 'No'}`;
                return false;
            }
            return true;
        }

        async function enumerateCameras() {
            if (!checkSecureContext()) return;

            try {
                await navigator.mediaDevices.getUserMedia({ video: true, audio: false });
                const devices = await navigator.mediaDevices.enumerateDevices();
                const videoDevices = devices.filter(d => d.kind === 'videoinput');

                cameraSelect.innerHTML = '';

                if (videoDevices.length === 0) {
                    cameraSelect.innerHTML = '<option value="">No cameras found</option>';
                    setStatus('No cameras found', 'error');
                    return;
                }

                videoDevices.forEach((device, index) => {
                    const option = document.createElement('option');
                    option.value = device.deviceId;
                    option.textContent = device.label || `Camera ${index + 1}`;
                    cameraSelect.appendChild(option);
                });

                selectedDeviceId = videoDevices[0].deviceId;
                setStatus(`Found ${videoDevices.length} camera(s)`, 'success');
            } catch (err) {
                console.error('Error enumerating cameras:', err);
                cameraSelect.innerHTML = '<option value="">Camera access denied</option>';
                setStatus('Camera access denied: ' + err.message, 'error');
            }
        }

        async function initWebcam() {
            if (!checkSecureContext()) return;

            try {
                if (stream) {
                    stream.getTracks().forEach(track => track.stop());
                }

                const res = resolutionPresets[resolutionSelect.value];
                currentResolution = { width: res.width, height: res.height };

                const constraints = {
                    video: {
                        width: { ideal: res.width },
                        height: { ideal: res.height }
                    },
                    audio: false
                };

                if (selectedDeviceId) {
                    constraints.video.deviceId = { exact: selectedDeviceId };
                }

                stream = await navigator.mediaDevices.getUserMedia(constraints);
                webcam.srcObject = stream;
                updateResolutionInfo();
                setStatus('Camera ready', 'success');
            } catch (err) {
                console.error('Webcam error:', err);
                setStatus('Failed to access camera: ' + err.message, 'error');
            }
        }

        function updateResolutionInfo() {
            const res = resolutionPresets[resolutionSelect.value];
            resolutionInfo.textContent = `Target: ${res.width}x${res.height} | Est. payload: ~${res.estKB}KB`;
        }

        cameraSelect.addEventListener('change', async () => {
            selectedDeviceId = cameraSelect.value;
            await initWebcam();
        });

        resolutionSelect.addEventListener('change', async () => {
            updateResolutionInfo();
            await initWebcam();
        });

        function captureFrame() {
            const res = resolutionPresets[resolutionSelect.value];
            canvas.width = res.width;
            canvas.height = res.height;

            const ctx = canvas.getContext('2d');
            ctx.drawImage(webcam, 0, 0, res.width, res.height);

            const quality = res.width <= 640 ? 0.85 : 0.75;
            const dataUrl = canvas.toDataURL('image/jpeg', quality);

            snapshot.src = dataUrl;
            snapshot.classList.remove('hidden');

            return dataUrl;
        }

        // ==================== Status & Stats ====================

        function setStatus(message, type = '') {
            statusBar.textContent = message;
            statusBar.className = 'status-bar';
            if (type) {
                statusBar.classList.add(type);
            }
        }

        function updateStats(stats) {
            if (stats.ttft !== undefined) {
                statTTFT.textContent = `${stats.ttft.toFixed(0)} ms`;
                statTTFT.className = 'stat-value' + (stats.ttft > 2000 ? ' warning' : stats.ttft > 5000 ? ' error' : '');
            }
            if (stats.latency !== undefined) {
                statLatency.textContent = `${(stats.latency / 1000).toFixed(2)} s`;
            }
            if (stats.tps !== undefined) {
                statTPS.textContent = stats.tps.toFixed(1);
                statTPS.className = 'stat-value' + (stats.tps > 20 ? ' highlight' : stats.tps < 5 ? ' warning' : '');
            }
            statRequests.textContent = requestCount.toString();
        }

        function resetStats() {
            statTTFT.textContent = '...';
            statLatency.textContent = '...';
            statTPS.textContent = '...';
            statTTSLatency.textContent = '--';
        }

        // ==================== Main LLM Function ====================

        async function sendToLLM() {
            if (isProcessing) {
                console.log('Already processing, skipping...');
                return;
            }

            const prompt = promptInput.value.trim();
            if (!prompt) {
                setStatus('Please enter a prompt', 'error');
                return;
            }

            // Reset TTS state for new request
            stopTTS();
            pendingSentence = '';
            ttsSequenceNumber = 0;
            nextExpectedSequence = 0;
            pendingAudioItems.clear();

            isProcessing = true;
            sendBtn.disabled = true;
            resetStats();
            setStatus('Capturing and sending...', 'loading');

            // Start thinking animation while waiting for LLM
            startThinkingAnimation();
            animateThinking();

            const startTime = performance.now();
            let firstTokenTime = null;
            let tokenCount = 0;

            try {
                const imageDataUrl = captureFrame();
                const apiUrl = apiUrlInput.value;
                const modelName = modelNameInput.value;

                const requestBody = {
                    model: modelName,
                    messages: [
                        {
                            role: "user",
                            content: [
                                {
                                    type: "image_url",
                                    image_url: { url: imageDataUrl }
                                },
                                {
                                    type: "text",
                                    text: prompt
                                }
                            ]
                        }
                    ],
                    max_tokens: 256,
                    temperature: 0.7,
                    stream: true
                };

                requestCount++;
                updateStats({});

                setStatus('Thinking...', 'loading');
                responseBox.innerHTML = '<span class="streaming-indicator"></span>';

                const response = await fetch(apiUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(requestBody)
                });

                if (!response.ok) {
                    const errorText = await response.text();
                    throw new Error(`API error ${response.status}: ${errorText}`);
                }

                const reader = response.body.getReader();
                const decoder = new TextDecoder();
                let fullResponse = '';

                while (true) {
                    const { done, value } = await reader.read();
                    if (done) break;

                    const chunk = decoder.decode(value, { stream: true });
                    const lines = chunk.split('\n');

                    for (const line of lines) {
                        if (line.startsWith('data: ')) {
                            const data = line.slice(6);
                            if (data === '[DONE]') continue;

                            try {
                                const json = JSON.parse(data);
                                const content = json.choices?.[0]?.delta?.content;
                                if (content) {
                                    if (firstTokenTime === null) {
                                        firstTokenTime = performance.now();
                                        const ttft = firstTokenTime - startTime;
                                        updateStats({ ttft });
                                        // Stop thinking animation when first token arrives
                                        stopThinkingAnimation();
                                        setStatus('Responding...', 'loading');
                                    }

                                    fullResponse += content;
                                    tokenCount++;
                                    responseBox.textContent = fullResponse;

                                    // Process for TTS (streaming)
                                    processTextForTTS(content, false);

                                    if (tokenCount % 5 === 0) {
                                        const elapsed = performance.now() - firstTokenTime;
                                        const tps = tokenCount / (elapsed / 1000);
                                        updateStats({ tps });
                                    }
                                }
                            } catch (e) {
                                // Ignore parse errors
                            }
                        }
                    }
                }

                // Final stats
                const endTime = performance.now();
                const totalLatency = endTime - startTime;
                const generationTime = firstTokenTime ? endTime - firstTokenTime : totalLatency;
                const finalTPS = tokenCount > 0 ? tokenCount / (generationTime / 1000) : 0;

                updateStats({ latency: totalLatency, tps: finalTPS });

                // Process remaining text for TTS
                await processTextForTTS('', true);

                if (!fullResponse) {
                    responseBox.textContent = 'No response received';
                }

                const timestamp = new Date().toLocaleTimeString();
                setStatus(`Response received at ${timestamp} (${tokenCount} tokens)`, 'success');

            } catch (err) {
                console.error('LLM error:', err);
                responseBox.textContent = 'Error: ' + err.message;
                setStatus('Error: ' + err.message, 'error');
                // Stop thinking animation on error
                stopThinkingAnimation();
            } finally {
                isProcessing = false;
                sendBtn.disabled = false;
                // Make sure thinking is stopped
                if (isThinking) {
                    stopThinkingAnimation();
                }
            }
        }

        // ==================== Autosend ====================

        function toggleAutosend() {
            if (isAutosending) {
                clearInterval(autosendInterval);
                autosendInterval = null;
                isAutosending = false;
                autosendBtn.textContent = 'Start Autosend';
                autosendBtn.classList.remove('btn-danger');
                autosendBtn.classList.add('btn-success');
                intervalInput.disabled = false;
                setStatus('Autosend stopped', 'success');
            } else {
                const interval = parseInt(intervalInput.value) || 5000;
                if (interval < 500) {
                    setStatus('Interval must be at least 500ms', 'error');
                    return;
                }

                isAutosending = true;
                autosendBtn.textContent = 'Stop Autosend';
                autosendBtn.classList.remove('btn-success');
                autosendBtn.classList.add('btn-danger');
                intervalInput.disabled = true;

                sendToLLM();
                autosendInterval = setInterval(() => {
                    if (!isProcessing && !isPlayingAudio) {
                        sendToLLM();
                    }
                }, interval);

                setStatus(`Autosend active (every ${interval}ms)`, 'success');
            }
        }

        // ==================== Event Listeners ====================

        // Initialize audio context on first user interaction (required for mobile)
        function initAudioOnInteraction() {
            initAudioContext();
            resumeAudioContext();
        }

        sendBtn.addEventListener('click', () => {
            initAudioOnInteraction();
            sendToLLM();
        });
        autosendBtn.addEventListener('click', () => {
            initAudioOnInteraction();
            toggleAutosend();
        });
        stopTtsBtn.addEventListener('click', stopTTS);
        unlockAudioBtn.addEventListener('click', unlockAudio);
        testAudioBtn.addEventListener('click', testTTSAudio);

        promptInput.addEventListener('keydown', (e) => {
            if (e.ctrlKey && e.key === 'Enter') {
                initAudioOnInteraction();
                sendToLLM();
            }
        });

        // ==================== Initialization ====================

        function setDefaultUrls() {
            const origin = window.location.origin;
            apiUrlInput.value = `${origin}/v1/chat/completions`;
            // TTS proxied through /tts/* on same origin
            ttsUrlInput.value = `${origin}/tts`;
        }

        async function init() {
            setDefaultUrls();
            await enumerateCameras();
            await initWebcam();
            // Start avatar animations
            startBlinkAnimation();
            startIdleAnimation();
            // Check audio status for mobile
            checkAudioStatus();
        }

        init();
    </script>
</body>
</html>
